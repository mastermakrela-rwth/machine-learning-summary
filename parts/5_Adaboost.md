# Adaboost

## Ensembles of Classifiers

### Idea

- Assume $K$ classifiers.
- They are independent.
- Each has error probability $p < 0.5$ on training data.

_Then:_
Majority vote of all classifiers should have a lower error than each individual classifier

### Constructing Ensembles

_How do we get different classifiers?_

#### Simplest Case\

1. Subsample the training data\
   \scriptsize Reuse the same training algorithm several times on different subsets of the training data.\normalsize

2. Train same classifier on different data.

=> Well-suited for “unstable” learning algorithms.

Where:

| Unstable                 | Stable            |
| ------------------------ | ----------------- |
| Decision trees           | Nearest neighbor  |
| neural networks          | linear regression |
| rule learning algorithms | SVMs              |

Unstable := small differences in training data can produce very different classifiers

#### Bagging\

Bagging := "Bootstrap aggregation"

1. In each run of the training algorithm,
   randomly select $M$ samples with replacement
   from the full set of $N$ training data points.

2. If $M = N$, then on average,
   63.2% of the training points will be represented.
   The rest are duplicates.

_Injecting randomness_

- Many (iterative) learning algorithms need a random initialization (e.g. k-means, EM)
- Perform multiple runs of the learning algorithm with different random initializations.

_Model Averaging_

- Suppose we have $H$ different models $h = 1,..., H$ with prior probabilities $p(h)$.

- Construct the marginal distribution over the data set: $p(X) = \sum_{h=1}^H p(X|h)p(h)$

_Interpretation_

- Just one model is responsible for generating the entire data set.
- The probability distribution over h just reflects our uncertainty which model that is.
- As the size of the data set increases, this uncertainty reduces,
  and $p(X|h)$ becomes focused on just one of the models.

_Model Combination_

- \scriptsize (e.g., Mixtures of Gaussians) \normalsize

- Different data points generated by different model components.
- Uncertainty is about which component created which data point.

-> One latent variable $z_n$, for each data point: $p(X) = \prod_{n=1}^N p(x_n) = \prod_{n=1}^N \sum_{z_n} p(x_n, z_n)$

#### Model Averaging: Expected Error\

```
Some math I don't understand. See slides 19-21 in VL 12.
```

_Average error of committee_

$\mathbb{E}_{COM} = \frac{1}{M} \mathbb{E}_{AV}$

This suggests that the average error of a model can be reduced by
a factor of M simply by averaging M versions of the model!

In reality the errors are **not** uncorrelated - usually highly correlated.

## Adaboost

### Idea

- Iteratively select an ensemble of component classifiers
- After each iteration, reweight misclassified training examples.
  - Increase the chance of being selected in a sampled training set.
  - Or increase the misclassification cost when training on the full set.

### Components

$h_m(x)$
~ "weak" / base classifier

$H(x)$
~ "strong" / final classifier

### Adaboost

Construct a strong classifier as a thresholded linear combination of the weighted weak classifiers:

$H(x) = sign( \sum_{m=1}^M \alpha_m h_m(x) )$

### Minimizing Exponential Error

Exponential error function: $E = \sum_{n=1}^N exp \{-t_n f_m(x_n) \}$

where $f_m(x)$ is a classifier defined as a linear combination of base classifiers $h_l(x)$:

$f_m(x) = \frac{1}{2} \sum_{l=1}^m \alpha_l h_l(x)$

_Goal:_

Minimize $E$ with respect to both the weighting coefficients $\alpha_l$
and the parameters of the base classifiers $h(x)$.

```
Here some step by step math that minimizes this.
```

### Final Algorithm

1. Initailization: $w_n^{(1)} = \frac{1}{N}$ for $n = 1,..., N$.

2. For $m = 1,..., M$ iterations

   a) Train a new weak classifier $h_m(x)$ using the current weighting
   coefficients $W(m)$ by minimizing the weighted error function:\
   $J_m = \sum_{n=1}^N w_n^{(m)} I(h_m(x) \neq t_n)$

   b) Estimate the weighted error of this classifier on $X$:\
   TODO

   c) TODO
   d) TODO

### Summary

#### Properties\

- Simple combination of multiple classifiers.
- Easy to implement.
- Can be used with many different types of classifiers.
  - None of them needs to be too good on its own.
  - In fact, they only have to be slightly better than chance.
- Commonly used in many areas.
- Empirically good generalization capabilities.

#### Limitations\

- Original AdaBoost sensitive to misclassified training data points.
  - Because of exponential error function.
  - Improvement by GentleBoost
- Single-class classifier
  - Multiclass extensions available
